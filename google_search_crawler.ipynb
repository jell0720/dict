{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_input(mlist):\n",
    "    chose_index = (mlist['crawl_mark'] == 'False') & (mlist['cleaned'].notnull())\n",
    "    return (mlist.loc[chose_index].index[0], mlist.loc[chose_index, 'cleaned'].iloc[0])\n",
    "\n",
    "def google_search_crawler(input_index, input_keyword, user_agents):\n",
    "    # input_keyword = '-4 fun'\n",
    "    # 組合URL\n",
    "    url = \"https://www.google.com.tw/search?q=\" + input_keyword + \"&start=0\"\n",
    "    user_agent = user_agents[random.randint(0, 9)]\n",
    "    # 請求\n",
    "    res = requests.get(url, user_agent)\n",
    "    # 解析\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    search_text = soup.find_all(\"div\", class_=\"g\") # 剝一層\n",
    "    # 開始訪問細項\n",
    "    title_list = list() # title\n",
    "    context_list = list() # context\n",
    "    cite_list = list() # cite\n",
    "    for result in search_text:\n",
    "        try: # title\n",
    "            title_list.append(result.find(class_='r').find('a').text)\n",
    "        except:\n",
    "            title_list.append(None)\n",
    "        try: # context\n",
    "            context_list.append(result.find(class_='st').text)\n",
    "        except:\n",
    "            context_list.append(None)\n",
    "        try: # cite\n",
    "            cite_list.append(result.find('cite').text)\n",
    "        except:\n",
    "            try: # google map沒有cite，要特別辨識\n",
    "                if result.find('img').get_attribute_list('id')[0] == 'lu_map':\n",
    "                    cite_list.append('map.google.com')\n",
    "                else:\n",
    "                    cite_list.append(None)\n",
    "            except:\n",
    "                cite_list.append(None)\n",
    "    input_text = [input_keyword for i in range(len(title_list))] # 產生等長的輸入文字\n",
    "    input_row = [input_index for i in range(len(title_list))] # 產生等長的原始index\n",
    "    '''\n",
    "    # 如果右邊有wiki\n",
    "    search_text[-1] # 基本在最後面\n",
    "    search_text[-1].find(class_='FSP1Dd').text # title\n",
    "    search_text[-1].find('span').text # context\n",
    "    tempp = re.sub('(\\\\n|\\.\\.\\.)', '', search_text[-1].get_text(separator=',')) #其他人也搜尋了\n",
    "    tempp = re.sub('.*也搜尋了,', '', tempp)\n",
    "    '''\n",
    "    # 創建矩陣，long表格\n",
    "    result = pd.DataFrame({'mlist_index': input_row, 'search_text': input_text,\n",
    "                           'result_title': title_list, 'result_context': context_list,\n",
    "                           'result_cite':cite_list})\n",
    "    result = result[['mlist_index', 'search_text', 'result_cite', 'result_title', 'result_context']]\n",
    "    \n",
    "    # 後修正，因為有些問題要直接用網頁結構拆有難度，等結果出來再修正\n",
    "    # 有\"圖片搜尋結果\"的頁面，把cite改成img.google\n",
    "    result.loc[result['result_title'].str.match('.*圖片搜尋結果$', na=False), 'result_cite'] = 'img.google.com'\n",
    "    # 右側有wiki的頁面，爬下來\n",
    "    if result.iloc[-1, 2:].isnull().all():\n",
    "        wiki_part = search_text[-1]\n",
    "        try:\n",
    "            result.iloc[-1, 2] = 'wikipedia.org'\n",
    "            result.iloc[-1, 3] = wiki_part.find(class_='FSP1Dd').text\n",
    "            result.iloc[-1, 4] = wiki_part.find('span').text.replace(' 維基百科', '')\n",
    "            \n",
    "            # 該block下方有\"其他人也搜尋了...\"\n",
    "            tempp = re.sub('(\\\\n|\\.\\.\\.)', '', search_text[-1].get_text(separator=',')) # 其他人也搜尋了\n",
    "            tempp = re.sub('.*搜尋了[\\u4e00-\\u9fa5]*,', '', tempp)\n",
    "            simi_temp = pd.DataFrame({'mlist_index': input_index, 'search_text': input_keyword,\n",
    "                                      'result_cite': 'similar', 'result_title': '知識圖譜相似',\n",
    "                                      'result_context': tempp}, index=[0])\n",
    "            result = pd.concat([result, simi_temp], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "    # 最下方的其他人也搜尋了\n",
    "    try:\n",
    "        others_search_string = soup.find_all('p', class_='aw5cc')\n",
    "        others_search_string = [temp.text for temp in others_search_string]\n",
    "        simi_temp = pd.DataFrame({'mlist_index': input_index, 'search_text': input_keyword,\n",
    "                                      'result_cite': 'similar', 'result_title': '相似搜尋字串',\n",
    "                                      'result_context': ','.join(others_search_string)}, index=[0])\n",
    "        result = pd.concat([result, simi_temp], axis=0, ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "    # sleep\n",
    "    time.sleep(1 + random.uniform(0, 1) + random.uniform(0, 1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# 請求頭\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20130406 Firefox/23.0', \\\n",
    "      'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:18.0) Gecko/20100101 Firefox/18.0', \\\n",
    "      'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/533+ \\\n",
    "      (KHTML, like Gecko) Element Browser 5.0', \\\n",
    "      'IBM WebExplorer /v0.94', 'Galaxy/1.0 [en] (Mac OS X 10.5.6; U; en)', \\\n",
    "      'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)', \\\n",
    "      'Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14', \\\n",
    "      'Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) \\\n",
    "       Version/6.0 Mobile/10A5355d Safari/8536.25', \\\n",
    "      'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "       Chrome/28.0.1468.0 Safari/537.36', \\\n",
    "      'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0; TheWorld)']\n",
    "# load mlist\n",
    "mlist = pd.read_csv('D:/iima/進行中/貼標工程/data/crawler/clean_mlist_for_gsearch_20190417.csv',\n",
    "                engine='python', encoding='UTF-8')\n",
    "mlist['crawl_mark'] = mlist['crawl_mark'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237129, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlist.head()\n",
    "mlist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mlist['crawl_mark'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, '富邦人壽')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chose_input(mlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 rows saved!\n",
      "200 rows saved!\n",
      "300 rows saved!\n",
      "400 rows saved!\n",
      "500 rows saved!\n",
      "600 rows saved!\n",
      "700 rows saved!\n",
      "800 rows saved!\n",
      "900 rows saved!\n",
      "1000 rows saved!\n",
      "1100 rows saved!\n",
      "1200 rows saved!\n",
      "1300 rows saved!\n",
      "1400 rows saved!\n",
      "1500 rows saved!\n",
      "1600 rows saved!\n",
      "1700 rows saved!\n",
      "1800 rows saved!\n",
      "1900 rows saved!\n",
      "2000 rows saved!\n",
      "2100 rows saved!\n",
      "2200 rows saved!\n",
      "2300 rows saved!\n",
      "2400 rows saved!\n",
      "2500 rows saved!\n",
      "2600 rows saved!\n",
      "2700 rows saved!\n",
      "2800 rows saved!\n",
      "2900 rows saved!\n",
      "3000 rows saved!\n",
      "3100 rows saved!\n",
      "3200 rows saved!\n",
      "3300 rows saved!\n",
      "3400 rows saved!\n",
      "3500 rows saved!\n",
      "3600 rows saved!\n",
      "3700 rows saved!\n",
      "3800 rows saved!\n",
      "3900 rows saved!\n",
      "4000 rows saved!\n",
      "4100 rows saved!\n",
      "4200 rows saved!\n",
      "4300 rows saved!\n",
      "4400 rows saved!\n",
      "4500 rows saved!\n",
      "4600 rows saved!\n",
      "4700 rows saved!\n",
      "4800 rows saved!\n",
      "4900 rows saved!\n",
      "5000 rows saved!\n",
      "5100 rows saved!\n",
      "5200 rows saved!\n",
      "5300 rows saved!\n",
      "5400 rows saved!\n",
      "5500 rows saved!\n",
      "5600 rows saved!\n",
      "5700 rows saved!\n",
      "5800 rows saved!\n",
      "5900 rows saved!\n",
      "藍新金流-mega 6medi\n",
      "6000 rows saved!\n",
      "6100 rows saved!\n",
      "6200 rows saved!\n",
      "6300 rows saved!\n",
      "6400 rows saved!\n",
      "6500 rows saved!\n",
      "6600 rows saved!\n",
      "6700 rows saved!\n",
      "6800 rows saved!\n",
      "6900 rows saved!\n",
      "7000 rows saved!\n",
      "7100 rows saved!\n",
      "7200 rows saved!\n",
      "7300 rows saved!\n",
      "7400 rows saved!\n",
      "7500 rows saved!\n",
      "7600 rows saved!\n",
      "7700 rows saved!\n",
      "7800 rows saved!\n",
      "7900 rows saved!\n",
      "8000 rows saved!\n",
      "8100 rows saved!\n",
      "8200 rows saved!\n",
      "8300 rows saved!\n",
      "8400 rows saved!\n",
      "8500 rows saved!\n",
      "8600 rows saved!\n",
      "8700 rows saved!\n",
      "8800 rows saved!\n"
     ]
    }
   ],
   "source": [
    "# 執行爬蟲\n",
    "all_result = pd.DataFrame(columns=['mlist_index', 'search_text', 'result_cite', 'result_title', 'result_context'])\n",
    "a = 0\n",
    "b = 0\n",
    "for i in range(len(mlist)):\n",
    "    input_tuple = chose_input(mlist)\n",
    "\n",
    "    try:\n",
    "        all_result = all_result.append(google_search_crawler(input_tuple[0], input_tuple[1], user_agents))\n",
    "        mlist.iloc[input_tuple[0], 2] = 'True'\n",
    "    except:\n",
    "        mlist.iloc[input_tuple[0], 2] = 'error'\n",
    "        print(input_tuple[1])\n",
    "    a += 1\n",
    "    if (a > 99) or (i == (len(mlist) -1)):\n",
    "        b += a\n",
    "        a = 0\n",
    "        print('%s rows saved!' % b)\n",
    "        # 存出\n",
    "        all_result.to_csv('D:/iima/進行中/貼標工程/data/crawler/output_google_search_result_20190419.csv', index=False, encoding='UTF-8', mode='a')\n",
    "        mlist.to_csv('D:/iima/進行中/貼標工程/data/crawler/output_clean_mlist_for_gsearch_20190419.csv', index=False, encoding='UTF-8')\n",
    "        all_result = pd.DataFrame(columns=['mlist_index', 'search_text', 'result_cite', 'result_title', 'result_context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlist['crawl_mark'].value_counts()\n",
    "[i for i in range(0, 10)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
